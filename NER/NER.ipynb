{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends = json.load(open(\"../trends-data/processed_trends.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 59.0/59.0 [00:00<00:00, 19.6kB/s]\n",
      "c:\\Users\\diasrodr\\Anaconda3\\envs\\webscrap\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\diasrodr\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 829/829 [00:00<00:00, 414kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 809kB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 799B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 36.8kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 433M/433M [00:33<00:00, 13.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9990139, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.999645, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Days until SpiderManAcrossTheSpiderVerse releases ',\n",
       " 'More explosive 2D FX I animated from the recent SpiderManAcrossTheSpiderVerse trailers Out very soon Spiderman2099 2DFX ',\n",
       " 'SonyPicturesUK So looking forward to this SpiderManAcrossTheSpiderVerse spiderverse',\n",
       " 'SpiderMan Across the SpiderVerse Trailer  SpidermanAcrossTheSpiderverse Spiderman Marvel Sony Trailer',\n",
       " 'When you have enough points to get the the Future Sense colllectors box for free Just pay shipping GFUEL SpiderMan SpiderManAcrossTheSpiderVerse GFuelEnergy GammaLabs ',\n",
       " 'SpiderManAcrossTheSpiderVerse will end on a great cliffhanger I was very satisfied after The Empire Strikes Back And hopefully This is our Empire Joaquim Dos Santos CoDirector ',\n",
       " 'deleted trailer shows new look at Ben Reilly aka the Scarlet Spider AcrossTheSpiderVerse SpiderManAcrossTheSpiderVerse SpiderVerse leak scarletspider benreilly itsv atsv intothespiderverse MilesMorales 3CFilmss SpiderLeaks ',\n",
       " 'PENI PARKER APPEAR IN ACROSS THE SPIDERVERSE but just not part in the Spider Society PeniParker PennyParker PenniParker  AcrossTheSpiderVerse SpiderMan SpiderManAcrossTheSpiderVerse SpiderVerse MilesMorales ',\n",
       " 'Hermione ISIS Spotify SpiderManAcrossTheSpiderVerse Danielle Disney Bad Bunny Harry Potter thicktrunktuesday Asians earlyrisersclub SuccessionHBO Thatcher Rodgers fyp BBCBreakfast  ',\n",
       " 'Hermione ISIS Spotify SpiderManAcrossTheSpiderVerse Danielle Disney Bad Bunny Harry Potter thicktrunktuesday Asians earlyrisersclub SuccessionHBO Thatcher Rodgers fyp BBCBreakfast  ',\n",
       " 'June is gonna be  Im gonna patiently wait for all the supposed comic book movie fatigue talk  SpiderManAcrossTheSpiderVerse Transformers TheFlash ',\n",
       " 'Hermione ISIS Spotify SpiderManAcrossTheSpiderVerse Danielle Disney Bad Bunny Harry Potter thicktrunktuesday Asians earlyrisersclub SuccessionHBO Thatcher Rodgers fyp BBCBreakfast  ',\n",
       " 'SpiderMan Across The SpiderVerse Inspired Graphics Card Bundle ZOTAC Gaming GeForce RTX AMP AIRO Amazon  ad SpiderManAcrossTheSpiderVerse SpiderMan ',\n",
       " 'Watch the final full trailer for SpiderManAcrossTheSpiderVerse  SpiderMan ',\n",
       " 'Hermione ISIS Spotify SpiderManAcrossTheSpiderVerse Danielle Disney Bad Bunny Harry Potter thicktrunktuesday Asians earlyrisersclub SuccessionHBO viral BBCBreakfast  TikTok ']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends[list(trends.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#SpiderManAcrossTheSpiderVerse'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trends.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "treds_entities = []\n",
    "for trend_topic in list(trends.keys()):\n",
    "    for trend in trends[trend_topic]:\n",
    "        entities = nlp(trend)\n",
    "        for ent in entities:\n",
    "            treds_entities.append({\"trend\":trend_topic, \"word\":ent[\"word\"], \"entity\":ent[\"entity\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "treds_entities = []\n",
    "for trend in trends[list(trends.keys())[0]]:\n",
    "    entities = nlp(trend)\n",
    "    for ent in entities:\n",
    "        treds_entities.append({\"word\":ent[\"word\"], \"entity\":ent[\"entity\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I-ORG     14656\n",
       "B-PER      8956\n",
       "I-PER      8945\n",
       "B-ORG      6414\n",
       "I-MISC     4463\n",
       "B-MISC     2234\n",
       "B-LOC      1399\n",
       "I-LOC      1089\n",
       "Name: entity, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.entity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diasrodr\\AppData\\Local\\Temp\\ipykernel_22732\\2727526669.py:5: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df_for_pinterest.groupby([\"trend\",'word']).agg({\"word\":\"count\"}).rename(columns={'word':'count'}).reset_index().groupby([\"trend\"])[\"word\",\"count\"].max(\"count\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trend</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#911LoneStar</td>\n",
       "      <td>Ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#911onFOX</td>\n",
       "      <td>Buck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#AdamKutnerPowerPlay</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#AgustD_SUGA_Tour_in_LA</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#AllForCITY</td>\n",
       "      <td>Matt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>hanbin</td>\n",
       "      <td>De</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>jack antonoff</td>\n",
       "      <td>Matt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>lorde</td>\n",
       "      <td>Sasha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>luke hemmings</td>\n",
       "      <td>Luke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>ted turner</td>\n",
       "      <td>Turner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>555 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       trend    word\n",
       "0               #911LoneStar      Ta\n",
       "1                  #911onFOX    Buck\n",
       "2       #AdamKutnerPowerPlay    Adam\n",
       "3    #AgustD_SUGA_Tour_in_LA       A\n",
       "4                #AllForCITY    Matt\n",
       "..                       ...     ...\n",
       "550                   hanbin      De\n",
       "551            jack antonoff    Matt\n",
       "552                    lorde   Sasha\n",
       "553            luke hemmings    Luke\n",
       "554               ted turner  Turner\n",
       "\n",
       "[555 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(treds_entities)#.groupby(\"trend\")[\"word\"].apply(set)\n",
    "df_for_pinterest = df[((df.entity == \"I-PER\") | (df.entity == \"B-PER\")) & ~(df.word.astype(str).str.startswith('#'))]\n",
    "# df_for_pinterest[\"pinterest\"] = df_for_pinterest.word + \" \" + df_for_pinterest.trend\n",
    "df_for_pinterest.groupby([\"trend\",\"word\"])[[\"word\"]].count()\n",
    "df_for_pinterest.groupby([\"trend\",'word']).agg({\"word\":\"count\"}).rename(columns={'word':'count'}).reset_index().groupby([\"trend\"])[\"word\",\"count\"].max(\"count\")\n",
    "df_for_pinterest.groupby(['trend'])['word'].apply(lambda x: x.value_counts().index[0]).reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 palavras com mais frequência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectors = vectorizer.fit_transform(raw_documents=trends['#911LoneStar'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['youve', 'girl', 'got'], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_top10_idx = df.max(axis=0).argsort()[:3]\n",
    "np.asarray(feature_names)[global_top10_idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
